test_pod:
  enabled: true
  image: bats/bats:1.8.2
  pullPolicy: IfNotPresent

loki:
  enabled: true
  image:
    repository: grafana/loki
    tag: 2.9.10        # 차트 2.9.12의 default: 2.6.1 (너무 구버전) # 구버전에선 필터링 이슈있음 # 3버전대로 마이그레이션시 추가작업 필요 
    pullPolicy: IfNotPresent
    
  # 클러스터 내부 Grafana에서 Default Datasource로 취급할것인가 여부
  isDefault: true
  # 클러스터 내부 Grafana에서 Loki Datasource를 프로비전하기 위한 ConfigMap에 사용됨. 외부노출시 사용불가.
  url: http://{{(include "loki.serviceName" .)}}:{{ .Values.loki.service.port }}
  readinessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45
  livenessProbe:
    httpGet:
      path: /ready
      port: http-metrics
    initialDelaySeconds: 45
  datasource:
    jsonData: "{}"
    uid: ""
  config:
    compactor:
      retention_enabled: true  # false면, 무제한 저장
    limits_config:
      retention_period: 168h    # default: 744h(31일), 최소 24h


promtail:
  enabled: true
  config:
    logLevel: info
    serverPort: 3101
    clients:
      # 로그 데이터를 전송할 주소. # Promtail은 Loki쪽으로 로그를 PUSH한다.
      - url: http://{{ .Release.Name }}:3100/loki/api/v1/push

  # PriorityClassName 리소스 선택 => daemonset들 간 우선순위 지정
  # 신규노드 생성시 promtail이 가장 먼저 켜져서 로그수집이 누락 방지
  priorityClassName: "system-node-critical"  # default: null
  
  # (default toleration 때문에) promtail DaemonSet의 스케줄링이 다른 DaemonSet보다 늦는 문제 
  # promtail이 critical addon 취급 못 받아 늦게 뜨고, 먼저 내려감 → node-exporter 수준 toleration으로 조정
  tolerations:
    - effect: NoSchedule
      operator: Exists
  
  # 다른 앱 부하에 의한 promtail 실행 이상 방지
  resources: # {}
    requests:
      cpu: 100m
      memory: 128Mi
    limits: {}
  # livenessProbe: 앱이 비정상으로 판단될시 재실행 (EKS에서 빈번히 신규노드 실행시 간헐적 실패케이스가 있는 듯하여 대응)
  # readiness: 이미 이 차트에선 default 설정이 있음. 하지만 readiness는 초기셋업 확인용으로, 이상동작시 종료,재실행 동작을 하진 않음
  livenessProbe: 
    httpGet:
      path: /ready
      port: 3101
    initialDelaySeconds: 15   # Running status 진입 후 카운트 시작
    periodSeconds: 5
    failureThreshold: 5

fluent-bit:
  enabled: false

grafana:
  enabled: true
  sidecar:
    datasources:
      label: ""
      labelValue: ""
      enabled: true
      maxLines: 1000
  image:
    tag: 11.5.1 # 8.3.5

  service:
    enabled: true
    type: LoadBalancer
    port: 80
    targetPort: 3000
      # targetPort: 4181 To be used with a proxy extraContainer
    ## Service annotations. Can be templated.
    annotations: {}
    labels: {}
    portName: service
    # Adds the appProtocol field to the service. This allows to work with istio protocol selection. Ex: "http" or "tcp"
    appProtocol: ""
    
  adminUser: admin
  adminPassword: admin

  persistence:
    type: statefulset  # helm uninstall 해도 남아있게 하려면 statefulset
    enabled: true

  ## Configure grafana dashboard providers
  ## ref: http://docs.grafana.org/administration/provisioning/#dashboards
  ##
  ## `path` must be /var/lib/grafana/dashboards/<provider_name>
  ##
  dashboardProviders: # {}
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'my-default'
          orgId: 1
          folder: 'my-folder'
          type: file
          disableDeletion: true
          editable: false
          options:
            path: /var/lib/grafana/dashboards/my-default

  ## Configure grafana dashboard to import
  ## NOTE: To use dashboards you must also enable/configure dashboardProviders
  ## ref: https://grafana.com/dashboards
  ##
  ## dashboards per provider, use provider name as key.
  ##
  dashboards: # {}
    my-default:
      logs-app:
        gnetId: 13639
        revision: 2
        datasource: Loki
      loki-kubernetes-logs:
        gnetId: 15141
        revision: 1
        datasource: Loki
      loki-k8sapp-infinitescroll:
        gnetId: 22874
        revision: 3
        datasource: Loki


prometheus:
  enabled: false
  isDefault: false
  url: http://{{ include "prometheus.fullname" .}}:{{ .Values.prometheus.server.service.servicePort }}{{ .Values.prometheus.server.prefixURL }}
  datasource:
    jsonData: "{}"

filebeat:
  enabled: false
  filebeatConfig:
    filebeat.yml: |
      # logging.level: debug
      filebeat.inputs:
      - type: container
        paths:
          - /var/log/containers/*.log
        processors:
        - add_kubernetes_metadata:
            host: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"
      output.logstash:
        hosts: ["logstash-loki:5044"]

logstash:
  enabled: false
  image: grafana/logstash-output-loki
  imageTag: 1.0.1
  filters:
    main: |-
      filter {
        if [kubernetes] {
          mutate {
            add_field => {
              "container_name" => "%{[kubernetes][container][name]}"
              "namespace" => "%{[kubernetes][namespace]}"
              "pod" => "%{[kubernetes][pod][name]}"
            }
            replace => { "host" => "%{[kubernetes][node][name]}"}
          }
        }
        mutate {
          remove_field => ["tags"]
        }
      }
  outputs:
    main: |-
      output {
        loki {
          url => "http://loki:3100/loki/api/v1/push"
          #username => "test"
          #password => "test"
        }
        # stdout { codec => rubydebug }
      }

# proxy is currently only used by loki test pod
# Note: If http_proxy/https_proxy are set, then no_proxy should include the
# loki service name, so that tests are able to communicate with the loki
# service.
proxy:
  http_proxy: ""
  https_proxy: ""
  no_proxy: ""
